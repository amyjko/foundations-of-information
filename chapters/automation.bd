The first time I saw a computer program was magical. It was 4rd grade and I was nine years old. There was a temporary installation of about fifteen Commodore 64 computers lined up in the hallway outside of our classroom. One class a time was invited out into the hallway to sit in pairs in plastic chairs. When my class was invited out, we walked single file to our seats and then our teacher, a very short and very cranky man, grumpily handed out a single piece of paper in front of each computer. He told us to follow the instructions on the sheet of paper, entering exactly what it said until we reached the end of the worksheet. The instructions were a cryptic list of pairs of numbers. We entered them, one by one, correcting our errors as we typed them in. When we reached the bottom of the list, we pressed the "Run" button on the keyboard.

To our surprise, a pixelated duck appeared! The numbers suddenly made sense: each one represented the position of one of square, and together, all of the position made up a picture of a duck. My partner and I immediately wanted to edit the points, seeing if I could make the duck's eyes bigger, or give it bigger wings, or better yet, change it into a monster or a cat. For some reason, the idea of telling a computer how to draw robotic, rectangular, black and white animals was far more interesting than just drawing animals myself, even though my hands could do far more with paper, pens, and paint.

We use code, now more than ever, automate how we create, retrieve, and analyze information. And yet, as in the story above, we often happily exchange the labor that we can do ourselves with the wonder of our minds and bodies with the speed, scale, logic, and versatility of code. In this chapter, we reflect on this trade, what we gain and lose when we make it, and the diverse consequences of shifting control over information and decisions to machines.

# When to automate?

Automation, of course, does not just include computing. As we noted in [Chapter 5|technology] in our discussion of information technology, we used mechanical devices to automate printing, phonographs to automate musical recordings, and electricity to automate the transmission of messages via telegraphs, phones, and television. And our discussion of what is gained and lost began with Socrates and his fears that writing itself was a form of "automation", in that it externalizes our memories, risking atrophy to our intelligence, memory, and wit. Code, therefore, is just the latest information technology to make us wonder about the tradeoffs of delegating our information labor to machines.

|Jackson.jpg|A photograph of Mary Winston Jackson.|Mary Winston Jackson, one of the computers that helped NASA get to space. She eventually became an aerospace engineer.|NASA|

When, then, is automation worth it? Let's examine this question by considering the many applications of code to problems of information. We'll begin with one of the first things that code automated: _calculation_. As you may recall, the first computers were people, in that humanity has performed the labor of arithmetic manually since arithmetic was invented. This was true even up through the Space Race in the mid-1950's, when the United States and the Soviet Union rushed to be the first to space. The calculations here were ballistic, involving algebra, geometry, and calculus, all in service of trying to aim and steer rockets in a manner that would allow them to escape orbit and safely return to Earth. Women, including many Black women mathematicians, performed the calculations that got the U.S into orbit<shetterly16>. When computers arrived shortly after, the need for human computers rapidly declined, leaving most out of work. They were replaced with a much smaller generation of computer programmers, who wrote programs to calculate trajectories instead of calculating them manually. 

What was gained? A new speed and complexity in space flight, requiring careful programming and planning. In fact, without code, little of the remaining missions to space would have been possible, as they all relied on onboard computers to track trajectories. What was lost was a career path for mathematicians, and their agility in responding to urgent needs for calculation in unexpected circumstances. And what remains is a public education system that still teaches the arithmetic used to get us to space.

|Cleary.jpg|A photograph of Beverly Cleary smiling on a couch.|Beverly Cleary, a children's librarian and one of history's most successful authors.|Christina Koci Hernandez|

Later, in the 1990's, there were fewer than 100 websites, and researchers pondered what the web might be. At the time, most of the valuable information in the world was in libraries, which archived books, newspapers, magazines, and other media. The people that made that information accessible were librarians<harris99>. Throughout history, the job of librarian have been diverse and ever changing: some librarians select books and other media for inclusion in archives, carefully reflecting on their credibility and significance; some librarians catalog and index, creating organizational schemes that help people browse and search collections; some librarians are experts in domains, helping people not only find answers to their questions, but find the right questions; some librarians teach literacy in schools, helping youth learn to read. It was in this context that the National Science Foundation, the primary research funding agency in the U.S., began investing in research on digital libraries. One project at Stanford funded a project that imagined how library collections might be digitized and search; one of the Ph.D. students on the project, Larry Page, imagined a future where all of the content in libraries was digitized, and developed a recursive algorithm to estimate page relevance, improving the relevance of rankings<page99>. With this algorithm, and the many algorithms for crawling, indexing, and retrieval that came before it, Page and his collaborator Sergey Brin co-founded Google, automating much of what librarians had been doing for centuries.

What was gained? Obviously, a transformation in our ability to find and retrieve documents stored on other people's computers. And when those documents have valuable content, this replicates the benefits of libraries, but does so at far greater speed, scale, and access than libraries had ever achieved. But what was lost was profound: libraries are institutions that celebrate equity, literacy, archiving, and truth. While accessing the information they have may be slower, Google has done little to adopt these values in supporting information archiving and retrieval. Half of the world lacks access to the internet, but most countries have public library systems open to all. Google has done little to address literacy, largely relying on schools and libraries to ensure literacy. Google largely ignores archiving, with the exception of Google Books, mostly documenting what is on the web now, and ignoring what used to be. And perhaps more importantly, Google has largely ignored truth, ignoring the critical role of libraries in archiving and curating credible information, and instead retrieving whatever is popular and current. What remains are two relatively independent institutions: a for-profit one that meets are immediate needs for popular information that has questionable truth, but offers little to address information literacy or inequity, and a not-for-profit one that continues to uphold these values, but struggles to retain public support because of its less than immediate response.

|Powells.jpg|A photo of Emily Powell in front of book shelves.|Emily Powell, chief executive of Powell's Bookstore, the world's largest independent bookstore in Portland, Oregon.|Leah Nash, NY Times|

Before the social web, social was personal. To hear about what our friends were doing, people had conversations with their friends. To get recommendations for books or movies, people might go to their local bookstore or library to get recommendations from avid readers, or read a review from a newspaper movie critic. To make food recommendations, people might spend months getting to know the owners of local restaurants, cafes, and diners, building an awareness of industry. Word of mouth, gossip, and sharing was relational and direct. As the social web emerged, algorithms began to mediate these relationships. We were more likely to learn about what our friends were doing because of a post that Facebook's news feed algorithm decided to recommend to us. Rather than relying on experts and enthusiasts to help us select media, we trusted collaborative filtering algorithms<resnick94> to model our collective interests, and give us more things that people "like" us seem to like. To decide where to eat, we began to rely on averages of star ratings from people we'd never met, of restaurants who decided to pay the Yelp tax<noble18>. Information about friends and where to shop no longer came from our friends and communities, but ranked, aggregated, models of our social networks and interests.

What was gained? It is certainly less work to keep up with our friends, and less work to decide what to read, watch, eat, and buy--especially social labor, as our interactions no longer need to involve people at all. What was lost were relationships, community, loyalty, and trust. The algorithms built to create digital recommendations are optimized to reduce our decision time, but not to connect us. And we do not yet know the implications of these lost connections on society: will our increased convenience and weakened community ties make us happier by making us more productive and satisfied, or was there something essential to community that we are losing?

|Parks.jpg|A photo of Nijeer Parks|Nijeer Parks, the third person known to be arrested for a crime he didn't commit based on a false positive face recognition match.|Mohamed Sadek, NY Times|

While the web began to mediate our connections, the policing in the United States was pondering code as well. Throughout U.S. history, a primary function of policing had been to restrict Black lives<browne15>. Policing emerged in the 1700's in South Carolina as slave patrols, which were groups of white men allowed to forcefully enter anyone's home if they were believed to be harboring people who had escaped bondage. As police departments emerged, they were overwhelmingly White and male, and charged with controlling the "dangerous underclass" of Black Americans, immigrants, and the poor. After the civil war, Jim Crow laws meant that police were charged with restricting Black American's voting rights, travel, and use of public spaces, while overlooking lynchings and mob murders. And after the Civil Right act, police have focused their patrols on Black neighborhoods for minor drug arrests, while largely ignoring the same violations happening on largely white college campuses. For policing, code was a way to optimize these efforts, prioritizing limited resources to predict crime. For example, the LA Police Department, in 2011, launched [Operation LASER|https://www.wired.com/story/los-angeles-police-department-predictive-policing/], to take data from past offenders in a two-year period, using it to prioritize where they patrol and who they surveil. Such practices increase arrests in those areas<brattingham18>. And when the accused are tried, and convicted, course are increasingly using racially-biased algorithms to predict the likelihood at the convicted person will commit future crimes<starr14>. Police departments are now adopting face recognition software racially biased against dark skin, leading to false arrests like that of [New Jersey man Nijeer Parks|https://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html?referringSource=articleShare], who was held in jail for 10 days and had to spend $5,000 in legal fees to prove his innocence. 

What is gained? For police departments, they may feel like they are better allocating their time, "optimizing" the number of arrests to reduce crime. What is lost a sense of freedom: Black people have always been surveilled in the United States<browne15>, but that surveillance was always by other (albeit sometimes racist) people. With data and cameras now constantly monitoring, and doing so in ways that are inaccurate and biased against their skin color, they have to live with being watched by both people _and_ technology, and with the fear that they might be unjustly arrested at any time because a training set in a machine vision algorithm didn't have enough Black faces to be accurate.

|Waymo.jpg|A photograph of two cars that have crashed into each other, one driverless.|A Waymo car failed to avoid an oncoming car that ran a red light.|Scripps Media, Inc.|

# How to automate?

In all of the stories above, there is a similar pattern: people had evolved practices over time to perform some information task, code was used to automate their work, and in the process, the humanity in the task was lost. But there is a pattern underneath these histories that goes deeper: it is the decision to delegate control over our gathering, analysis, and interpretation of information from human, subjective, emotional, and relational human processes to procedural, objective, rational, impersonal computational processes. In that decision is a choice about precisely what aspects of human experience we delegate to processing information, and what new kinds of partnerships we form between people and information technology to help us.

The dawn of computing set up a continuum for these choices. On one end was ~automation~automation. This vision--championed by researchers like Marvin Minsky, who is often called the "father of artificial intelligence"--imagined a world in which computers would replicate key aspects of human intelligence such as search, pattern recognition, learning, and planning<minsky61>. In this future, computers would replace people, and even achieve a degree of autonomy. It is this same vision that has led to dreams of driverless cars that transport us to our destinations and autonomous robots that serve and care for us. While these visions are still quite far from being a reality<dietterich17>, they continue to capture our imaginations in science fiction, and drive massive public and private investments. They are also the foundation of much of the disruption in the stories above.

|Arm.jpg|Les Baugh with his two robotic arms.|Les Baugh tests two robotic prosthetics that he controls with his mind.|NY Times|

The counter narrative to automation was one of ~augmentation~augmentation. This vision--championed by people like Vannevar Bush<bush45> and Douglas Englebart, and carried forward through fields like Human-Computer Interaction--imagined a world in which computers did not replace people, but enhance their abilities. In this future, computing is designed not to do our work for us, or make our decisions, but to augment our ability to do these things. Unlike AI, which has struggled to realize its strongest visions of autonomy, augmentation is the reality we live with today: computers have enhanced our ability to access information, to create, and to connect in so many ways. As the capability of computing has expanded, this has led to refinements of augmentation narratives, such as _human-computer integration_<farooq16>, which imagines computers as more than just enhancing our abilities, but becoming proactive, symbiotic partners in our lives.

Of course, the dichotomy between automation and augmentation is a false one. Computers will likely never be completely independent of humanity, as they will always require us to shape their behavior and intelligence. And as much as we enhance ourselves with computing, we will at some biological level likely always be human, with both our rational minds, and our emotional ones. And in both visions of automation, there is little attention to the inequities and injustices in society that underlie how we create information technology<benjamin19>, and how we deploy it into the world<oneil16,noble18>. The truth is that we have always had to make challenging moral, ethical, and sociopolitical choices about what power we give people and what power we give technology. Sometimes those the consequences of those choices are not clear, and some of us end up with enriched lives, and others end up abused or oppressed. The challenge is trying to know these consequences upfront, before we decide whether and how to automate, and then having the will to do what we think is right.

# Podcasts

* [What happens when an algorithm gets it wrong, In Machines We Trust, MIT Technology Review|https://www.technologyreview.com/2020/08/12/1006636/face-recognition-algorithm-false-arrest-police-robert-williams/]. Discusses a false arrest based in racially biased facial recognition software.

* [AI in the Driver's Seat, In Machines We Trust, MIT Technology Review|https://www.technologyreview.com/2020/09/24/1008878/why-people-might-never-use-autonomous-cars/]. Discusses the many complexities in human-machine communication that have been largely ignored in the design and engineering of current driverless car technology.

* [She's Taking Jeff Bezos to Task, Sway, NY Times|https://www.nytimes.com/2021/04/19/opinion/sway-kara-swisher-joy-buolamwini.html]. An interview with Joy Buolamwini, an activist who leads the Algorithmic Justice League, about facial recognition, algorithmic bias, corporate resistance, and opportunities for AI legislation.

* [What's Causing the Tesla Crashes, What Next:TBD, Slate|https://slate.com/podcasts/what-next-tbd/2021/04/tesla-autopilot]. An interview with Missy Cummings, a safety critical systems researcher at Duke, about driverless cars.

* [Biased Algorithms, Biased World, On the Media|https://www.wnycstudios.org/podcasts/otm/episodes/biased-algorithms-biased-world-on-the-media]. Discusses the illusion of algorithmic objectivity and how they end up reflecting the biases in our world.

* [An Engineer Tries to Build His Way Out of Tragedy, The Experiment|https://www.theatlantic.com/podcasts/archive/2022/03/traumatic-brain-injury-rehab-engineering/627089/]. Discusses the limits of solutionism when facing lived experiences.